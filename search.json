[
  {
    "objectID": "posts/assignment3/index.html",
    "href": "posts/assignment3/index.html",
    "title": "Assignment 3 – Brad Stenger",
    "section": "",
    "text": "1-2. Create a problem in your field similar to the one that starts on page 46 (Fig. 2.1.11) in Class Handout #2\n“The probability of the occurrence of either one event or another or both is the sum of their individual probabilities minus the probability of their joint occurrence.”\nThe throw in football passing plays are either shorter or longer than the distance required for a first down.\n\n\\(\\mathrm{prob\\ of\\ catch\\ when\\ } xyac_{fd} &gt; 0.99 =&gt; who\\)\n\\(\\mathrm{prob\\ of\\ catch\\ when\\ } xyac_{fd} &lt; 0.99 * prob\\ expected_{fd}\\)\n\\(x:play\\_type == pass \\ \\&\\& \\ \\mathrm{prob\\ of\\ negative\\ epa\\ } (epa &lt; 0)\\)\n\nThe quarterback of a football has up to six eligible receivers on a passing play. There are also two distinct scenarios on a passing play. Throw past the first down distance so that a catch ensures a first down. Or throw short of first down distance and leave the receiver work to do (in the form of yards after catch, yac) to gain the first down. Each receiver has a probability for getting a first down when the throw distance is less than first down distance (lt_fd) and another probability when the throw distance is greater than first down distance (gt_fd). These probabilities are mutually exclusive but not independent and the added probabilities (pr_gt_fd + pr_lt_fd) is the net probability for a targeted receiver getting a first down.\nThe probability of a fumble by the receiver after a catch is, however, independent. Lost fumble turnovers are rare but costly occurrences for the football offense. The best NFL teams, in many cases, minimize their turnovers.\n\nlibrary(gt)\n\n\n#\\| nflfastr\n\nnflfastR::load_pbp(2025) |&gt; \n  dplyr::group_by(receiver, receiver_id, posteam) |&gt; \n  dplyr::filter(play_type == \"pass\", air_yards &lt; yardline_100, posteam == \"DET\") |&gt; \n  dplyr::mutate(neg_play = ifelse(\n    sack == 1, 1, ifelse(\n      fumble_lost == 1, 1, ifelse(\n        interception == 1, 1, 0)))) |&gt; \n  dplyr::mutate(exp_fd = ifelse(xyac_fd &gt;= 0.99, 1, 0)) |&gt; \n  dplyr::mutate(actual_exp_fd = ifelse(xyac_fd &gt;= 0.99 & first_down, 1, 0)) |&gt; \n  dplyr::mutate(not_fd = ifelse(xyac_fd &lt; 0.99, 1, 0)) |&gt; \n  dplyr::mutate(actual_not_fd = ifelse(xyac_fd &lt; 0.99 & first_down, 1, 0)) |&gt; \n  dplyr::summarize( \n    passes = dplyr::n(), \n    neg_pct = sum(neg_play) / dplyr::n(), \n    gt_com = sum(exp_fd*(complete_pass)), \n    gt_fd = sum(exp_fd, na.rm=TRUE), \n    actual_gt_fd = sum(actual_exp_fd, na.rm=TRUE), \n    gt_fd_pct = actual_gt_fd / gt_fd, \n    lt_com = sum(not_fd*(complete_pass)), \n    lt_fd = sum(not_fd, na.rm=TRUE), \n    actual_lt_fd = sum(actual_not_fd, na.rm=TRUE), \n    lt_fd_pct = actual_lt_fd / lt_fd ) |&gt; \n  dplyr::ungroup() |&gt; \n  dplyr::select(receiver, posteam, passes, neg_pct, gt_com, gt_fd, actual_gt_fd, gt_fd_pct, lt_com, lt_fd, actual_lt_fd, lt_fd_pct) |&gt; \n  gt()\n\n`summarise()` has grouped output by 'receiver', 'receiver_id'. You can override\nusing the `.groups` argument.\n\n\n\n\n\n\n\n\nreceiver\nposteam\npasses\nneg_pct\ngt_com\ngt_fd\nactual_gt_fd\ngt_fd_pct\nlt_com\nlt_fd\nactual_lt_fd\nlt_fd_pct\n\n\n\n\nA.Firkser\nDET\n2\n0.00000000\n0\n0\n0\nNaN\n2\n2\n1\n0.5000000\n\n\nA.St. Brown\nDET\n110\n0.02727273\n26\n46\n26\n0.5652174\n52\n64\n18\n0.2812500\n\n\nB.Wright\nDET\n21\n0.04761905\n0\n0\n0\nNaN\n13\n21\n6\n0.2857143\n\n\nD.Montgomery\nDET\n25\n0.00000000\n2\n3\n2\n0.6666667\n19\n22\n7\n0.3181818\n\n\nI.TeSlaa\nDET\n8\n0.00000000\n3\n6\n3\n0.5000000\n1\n2\n0\n0.0000000\n\n\nJ.Gibbs\nDET\n66\n0.00000000\n3\n7\n3\n0.4285714\n55\n59\n19\n0.3220339\n\n\nJ.Williams\nDET\n71\n0.01408451\n26\n44\n26\n0.5909091\n19\n27\n9\n0.3333333\n\n\nK.Raymond\nDET\n19\n0.00000000\n4\n5\n4\n0.8000000\n11\n14\n3\n0.2142857\n\n\nR.Dwelley\nDET\n4\n0.00000000\n0\n0\n0\nNaN\n2\n4\n0\n0.0000000\n\n\nS.LaPorta\nDET\n48\n0.00000000\n12\n17\n12\n0.7058824\n27\n31\n10\n0.3225806\n\n\nT.Kennedy\nDET\n4\n0.00000000\n1\n1\n1\n1.0000000\n3\n3\n1\n0.3333333\n\n\nNA\nDET\n18\n0.00000000\nNA\n0\n0\nNaN\nNA\n0\n0\nNaN\n\n\n\n\n\n\n\nIn the table “gt_” refers to passes that travel greater than the distance to the first down. “lt_” refers to passes that travel less than the distance to the first down. “neg_pct” refers to percent of passes that are caught and then fumbled to the other team. The principal receivers for DET are A.St.Brown (ASB), J.Gibbs (JG), J.Williams (JW) and S.LaPorta (SLP).\n\ndata.frame( receiver = c(\"A.St.Brown\",\"J.Gibbs\",\"J.Williams\",\"S.LaPorta\"), abbrev = c(\"ASB\", \"JG\", \"JW\", \"SLP\"), gt_fd = c(0.79, 0.33, 0.47, 0.69), lt_fd = c(0.35, 0.18, 0.55, 0.26), fumble = c(0.02, 0, 0.03, 0) ) |&gt; \n  gt()\n\n\n\n\n\n\n\nreceiver\nabbrev\ngt_fd\nlt_fd\nfumble\n\n\n\n\nA.St.Brown\nASB\n0.79\n0.35\n0.02\n\n\nJ.Gibbs\nJG\n0.33\n0.18\n0.00\n\n\nJ.Williams\nJW\n0.47\n0.55\n0.03\n\n\nS.LaPorta\nSLP\n0.69\n0.26\n0.00\n\n\n\n\n\n\n\n“epa” stands for Expected Points Added. Fumble turnovers and first downs have an average epa.\n\nnflfastR::load_pbp(2025) |&gt; \n  dplyr::filter(play_type == \"pass\", air_yards &lt; yardline_100, fumble_lost == 1 | first_down == 1) |&gt; \n  dplyr::summarize( epa_fumble = sum(epa * fumble_lost) / sum(fumble_lost), epa_fd = sum(epa* first_down) / sum(first_down)) |&gt; \n  gt()\n\n\n\n\n\n\n\nepa_fumble\nepa_fd\n\n\n\n\n-4.046483\n1.631217\n\n\n\n\n\n\n\n\n\\(\\mathrm{expected\\ return\\ on\\ throw\\ beyond\\ first\\ down} = P[\\mathrm{gt\\_fd}] * epa_{fd} + P[\\mathrm{fumble}] * epa_{fumble}\\)\n\\(\\mathrm{expected\\ return\\ on\\ throw\\ short\\ of\\ first\\ down} = P[\\mathrm{lt\\_fd}] * epa_{fd} + P[\\mathrm{fumble}] * epa_{fumble}\\)\n\n\n\n\nreceiver\ngt_fd epa\nlt_fd epa\n\n\n\n\nASB\n1.18\n0.48\n\n\nJG\n0.52\n0.28\n\n\nJW\n0.63\n0.76\n\n\nSLP\n1.10\n0.46\n\n\n\nDetroit has two receivers, A.St.Brown and S.LaPorta, who are good at getting first downs on longer throws and on shorter throws. J.Williams is a threat on short throws but struggles to catch longer throws. J.Gibbs is a reliable target on shorter throws but converts a small percentage of those passes into first downs.\n\n\n3-4. Create a problem in your field similar to the one presented on page 56 (Fig. 2.1.15) in Class Handout #2\nThe application of survival analysis in sports does not use a time-based probability of occurrence like the class example does.\nhttps://www.nature.com/articles/s41598-021-87920-6, “Temporal trends in incidence of time-loss injuries in four male professional North American sports over 13 seasons”\n64.7 injuries for 1000 exposures in NFL games during one NFL team 10-year pre-season totals (https://journals.sagepub.com/doi/10.1177/0363546508316021, Epidemiology of National Football League Training Camp Injuries from 1998 to 2007)\n44.8 injuries for 1000 exposures for NCAA Division 1 football games between 2014-15 and 2018-19 based onEpidemiology of Injuries in National Collegiate Athletic Association Men’s Football: 2014-2015 Through 2018-2019 in Journal of Athletic Training (July 2021).\nOne NFL game counts as one injury exposure, so a rough estimate for a single player experiencing an injury in a game is approximately is 0.005 or 0.5%.\nEstimates vary for NFL players’ career length. An old but still often-used figure is 3.3 years across all players. Sports participation and achievement is often governed by Pareto (80-20) or Pareto-like distributions. Depending on player ability, the careers of quality NFL players is more likely 8, 12 or 15 years long. Obsolescence would not seem to be well represented by retirement in professional NFL football.\nAn alternative is to estimate the peak performance years for NFL players and then use aging curves to describe players’ post-peak decline. Pro Football Focus analyst Timo Riske describes the peak value years for NFL players (averaged across positions) is 26-28 before player values drops at more rapid rates. This sets up a survivor model where the two events are (1.) injury and (2.) rapid post-peak decline. Like the class example, each event is a distinct path to the same endpoint, which is the end of a professional football playing career.\nThe model has:\n\nInitial value based on talent projection based on draft round or salary (though NFL draft position has been shown to be an inaccurate indicator of a football player’s realized value.)\nAi = [injury occurs in game i] =&gt;\n\\[\np = 0.005\n\\]\nBi = [positive or negative player development or decline in game i] =&gt; r … positive development (negative decline) more likely during early-career and less likely during late-career … so start at Game 70, which is 4-5 seasons into an NFL career.\n\\[\nr_i = 1 - e^{-0.001i}\n\\]\n\n\\[ P[\\mathrm{development/decline\\ through}\\ j] = (1-p)^{i} \\prod_{i=1}^{j-1}(1-r_{i}) \\]\n\\[ P[\\mathrm{career\\ ends\\ in\\ game}\\ j] = [p + r_j - pr_j](1-p)^{i} \\prod_{i=1}^{j-1}(1-r_{i}) \\]\nFirst, let’s inspect the probability for a player’s decline that comes after their initial early-career development period.\n\nJ &lt;- 70:(70 + (6*17)) \ngames &lt;- c() \nsurv &lt;- c() \nfor (j in J) { \n  game &lt;- j - 69 \n  games &lt;- c(games, game) \n  prob &lt;- (1 - 0.995*exp(-0.001*game))*(0.995^(game-1))*prod(exp(-0.001*games)) \n  surv &lt;- c(surv, prob) \n} \nchart_data &lt;- rbind(surv) \nbarplot(chart_data, names.arg = games, xlab = \"Games Played\", ylab = \"P(development/decline)\")\n\n\n\n\n\n\n\n\nThe barplot is consistent with this 2023 graph of aging curves based on players’ fantasy football scoring by The Washington Post.\n\nSecond, the probabilities in an injury occurrence survival analysis during the same games sequence shows tiny probabilities.\n\nJ &lt;- 70:(70 + (6*17)) \ngames &lt;- c() \nsurv &lt;- c() \nchart_data &lt;- c()\nfor (j in J) { \n  game &lt;- j - 69 \n  games &lt;- c(games, game) \n  prob &lt;- ((0.005 + (1-exp(-0.001*max(J)))-(0.005)*(1-exp(-0.001*max(J))))^game)*(1-exp(-0.001*game))\n  surv &lt;- c(surv, prob) \n}\nchart_data &lt;- rbind(surv) \n#barplot(chart_data)\nbarplot(chart_data, names.arg = games, xlab = \"Games Played\", ylab = \"P(injury)\")"
  },
  {
    "objectID": "posts/course-project/index.html",
    "href": "posts/course-project/index.html",
    "title": "project",
    "section": "",
    "text": "Survival analysis and athlete aging curves\nThe application of survival analysis in sports does not use a time-based probability of occurrence like many engineering risk examples do. A study by Bullock et al. in Nature Scientific Reports (April 2021) counted injuries where athletes missed games in the four major U.S. professional sports (baseball, basketball, football, hockey) between 2007-2019. They found for 54,944 total injuries there were an average of 62.49 injuries per 100 players per season.\nMore often injury occurrences are counted by exposures where a single exposure is either a practice or game. Other reputable papers count 64.7 injuries for 1000 exposures in NFL games during one NFL team 10-year pre-season totals in Epidemiology of National Football League Training Camp Injuries from 1998 to 2007 in The American Journal o f Sports Medicine (April 2008). There were 44.8 injuries for 1000 exposures for NCAA Division 1 football games between 2014-15 and 2018-19 based onEpidemiology of Injuries in National Collegiate Athletic Association Men’s Football: 2014-2015 Through 2018-2019 in Journal of Athletic Training in (July 2021).\nOne NFL game counts as one injury exposure, so a rough estimate for a single player experiencing an injury in a game is approximately is 0.005 or 0.5%.\nEstimates vary for NFL players’ career length. An old but still often-used figure is 3.3 years across all players. Sports participation and achievement is often governed by Pareto (80-20) or Pareto-like distributions. Depending on player ability, the careers of quality NFL players is more likely 8, 12 or 15 years long. Obsolescence would not seem to be well represented by retirement in professional NFL football.\nAn alternative is to estimate the peak performance years for NFL players and then use aging curves to describe players’ post-peak decline. Pro Football Focus analyst Timo Riske describes the peak value years for NFL players (averaged across positions) is 26-28 before player values drops at more rapid rates. This sets up a survivor model where the two events are (1.) injury and (2.) rapid post-peak decline. Like the class example, each event is a distinct path to the same endpoint, which is the end of a professional football playing career.\nThe model has:\n\nInitial value based on talent projection based on draft round or salary (though NFL draft position has been shown to be an inaccurate indicator of a football player’s realized value.)\nAi = [injury occurs in game i] =&gt;\n\\[\np = 0.005\n\\]\nBi = [positive or negative player development or decline in game i] =&gt; r … positive development (negative decline) more likely during early-career and less likely during late-career … so start at Game 70, which is 4-5 seasons into an NFL career.\n\\[\nr_i = 1 - e^{-0.001i}\n\\]\n\n\\[ P[\\mathrm{development/decline\\ through}\\ j] = (1-p)^{i} \\prod_{i=1}^{j-1}(1-r_{i}) \\]\n\\[ P[\\mathrm{career\\ ends\\ in\\ game}\\ j] = [p + r_j - pr_j](1-p)^{i} \\prod_{i=1}^{j-1}(1-r_{i}) \\]\nFirst, let’s inspect the probability for a player’s decline that comes after their initial early-career development period.\n\nJ &lt;- 70:(70 + (6*17)) \ngames &lt;- c() \nsurv &lt;- c() \nfor (j in J) { \n  game &lt;- j - 69 \n  games &lt;- c(games, game) \n  prob &lt;- (1 - 0.995*exp(-0.001*game))*(0.995^(game-1))*prod(exp(-0.001*games)) \n  surv &lt;- c(surv, prob) \n} \nchart_data &lt;- rbind(surv) \nbarplot(chart_data, names.arg = games, xlab = \"Games Played\", ylab = \"P(development/decline)\")\n\n\n\n\n\n\n\n\nThe barplot is consistent with this 2023 graph of aging curves based on players’ fantasy football scoring by The Washington Post.\n\nSecond, the probabilities in an injury occurrence survival analysis during the same games sequence shows tiny probabilities.\n\nJ &lt;- 70:(70 + (6*17)) \ngames &lt;- c() \nsurv &lt;- c() \nchart_data &lt;- c()\nfor (j in J) { \n  game &lt;- j - 69 \n  games &lt;- c(games, game) \n  prob &lt;- ((0.005 + (1-exp(-0.001*max(J)))-(0.005)*(1-exp(-0.001*max(J))))^game)*(1-exp(-0.001*game))\n  surv &lt;- c(surv, prob) \n}\nchart_data &lt;- rbind(surv) \n#barplot(chart_data)\nbarplot(chart_data, names.arg = games, xlab = \"Games Played\", ylab = \"P(injury)\")\n\n\n\n\n\n\n\n\n\n\nDemand-to-Capacity Model: Capacity\nAthletes, regardless of sport, age, or size have a finite Capacity. Repetive stresses wear down soft tissues. Collisions have the potential to further compromise soft tissues. Exercise scientists at the University of North Carolina published methodology to measure accumulated wear and tear on athletes based on player tracking data. Player tracking uses GPS or, if indoor, radio tracking to capture an X-Y position for every player on the court or field location, taking measurements at 10-30 frames per second.\nMany college and professional sports gather player tracking data. The NFL has gathered this type of data for years. The NFL also organizes an annual data science analytics competition called the Big Data Bowl which asks the sports data science community to contribute insights based on football player tracking data.\nThe original method for gauging athlete Capacity is a measurement called “training load.” The number that training load represents is an indication of the accumulated work by an athlete during a day of sports training, practice or competition. By itself the work by athlete in a single day is not very useful. Sports, especially at its highest levels, involves repetitive stresses, like with running but encompassing a wide range of movement patterns necessary for a particular sport. The preferred method for determining when training load is approaching Capacity is something called Acute-Chronic Workload Ratio (ACWR), putting the short-term training load in the numerator (acute) and the longer-term training load in the denominator (chronic). A 2020 commentary by Impellizzeri et al. in Journal of Orthopaedic & Sports Physical Therapy debunked the practice, writing that higher training loads increased the number of injury exposures (remember from before) and there was no causal evidence for more injuries from higher training loads in the numerator of a high ACWR.\nThe opportunity to get a handle on athletic Capacity goes through player tracking, merited in part by the attention given to the recent UNC paper. Theoretically, visualizing player tracking data stands to produce a biomechanical movement signature. The insight of the UNC paper is that changes in direction, accurately captured with player tracking, are substantially more wear and tear than unencumbered straight-ahead running. The collisions that are also present in football player tracking data should also be additive in terms of athlete wear and tear.\n\nd3 = require(\"d3@7\")\nreceivers = FileAttachment(\"receivers.json\").json()\nreceivers_offset = FileAttachment(\"receivers_offset.json\").json()\nr_offset = d3.group(\n  receivers_offset,\n  (d) =&gt; d.nfl_id,\n  (d) =&gt; d.play_id\n)\ncollisions = FileAttachment(\"collisions.json\").json()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nchart = {\n  // Declare the chart dimensions and margins.\n  const marginTop = 20;\n  const marginRight = 1;\n  const marginBottom = 40;\n  const marginLeft = 40;\n  const rowHeight = 22;\n  const width = 1200;\n  const height =\n    rowHeight * Array.from(new Set(d3.map(receivers, (d) =&gt; d.nfl_id))).length +\n    marginTop +\n    marginBottom;\n\n  // Create the scales.\n  const x = d3\n    .scaleLinear()\n    // max value from receievers_offset.csum\n    .domain([1, d3.max(receivers_offset, (d) =&gt; d.csum)])\n    .rangeRound([marginLeft, width - marginRight]);\n\n  const y = d3\n    .scaleBand()\n    .domain(Array.from(new Set(d3.map(receivers, (d) =&gt; d.nfl_id))))\n    .rangeRound([marginTop, height - marginBottom]);\n\n  const color = d3.scaleSequentialSqrt(\n    [0, d3.max(receivers, (d) =&gt; d.dist)],\n    d3.interpolateHsl(\"#ddd\", \"red\")\n  );\n\n  // Create the SVG container.\n  const svg = d3\n    .create(\"svg\")\n    .attr(\"viewBox\", [0, 0, width, height])\n    .attr(\"viewBox\", [0, 0, width, height])\n    .attr(\"width\", width)\n    .attr(\"height\", height)\n    .attr(\"style\", \"max-width: 100%; height: auto;\");\n\n  // Append the axes.\n  svg\n    .append(\"g\")\n    .attr(\"transform\", `translate(0,${height - marginBottom})`)\n    .call(d3.axisBottom(x).ticks(width / 80))\n    .call((g) =&gt; g.select(\".domain\").remove())\n    .call((g) =&gt;\n      g\n        .append(\"text\")\n        .attr(\"x\", width)\n        .attr(\"y\", marginBottom - 4)\n        .attr(\"fill\", \"currentColor\")\n        .attr(\"text-anchor\", \"end\")\n        .text(\"frame →\")\n    );\n\n  svg\n    .append(\"g\")\n    .attr(\"class\", \"grid\")\n    .attr(\"transform\", `translate(${marginLeft},0)`)\n    .call(d3.axisLeft(y).tickSize(-width))\n    .call((g) =&gt; g.select(\".domain\").remove())\n    .call((g) =&gt;\n      g\n        .append(\"text\")\n        .attr(\"x\", -marginLeft)\n        .attr(\"y\", 10)\n        .attr(\"fill\", \"currentColor\")\n        .attr(\"text-anchor\", \"start\")\n        .text(\"↑ player\")\n    );\n\n  // visuals\n  // tracking paths with velocity as grey-red gradient\n  svg\n    .append(\"g\")\n    .selectAll(\"path\")\n    .data(receivers)\n    .join(\"path\")\n    .attr(\"transform\", (d) =&gt;\n      isNaN(d.dist)\n        ? `translate(${x(\n            d.frame_id + r_offset.get(d.nfl_id).get(d.play_id)[0].offset\n          )}, ${y(d.nfl_id)}) scale(0.1,3)`\n        : `translate(${x(\n            d.frame_id + r_offset.get(d.nfl_id).get(d.play_id)[0].offset\n          )}, ${y(d.nfl_id)}) rotate(${d.dir}) scale(${0.2 + d.dx},${1 + d.dy})`\n    )\n    .attr(\"fill\", (d) =&gt;\n      isNaN(d.dist) ? \"#000\" : color(d.dist)\n    )\n    .attr(\"d\", d3.symbol(d3.symbolTriangle));\n\n  // collisions with black filled circles\n  svg\n    .append(\"g\")\n    .selectAll(\"path\")\n    .data(collisions)\n    .join(\"path\")\n    .attr(\n      \"transform\",\n      (d) =&gt;\n        `translate(${x(\n          d.frame_id + r_offset.get(d.nfl_id).get(d.play_id)[0].offset\n        )}, ${y(d.nfl_id)}) scale(0.5,0.5)`\n    )\n    .attr(\"fill\", \"#000\")\n    .attr(\"d\", d3.symbol(d3.symbolCircle));\n\n  return svg.node();\n}\n\n\n\n\n\n\nThe graph shows the 60 most used receivers in games during the first NFL weekend of 2017. Blobs are bright red when the players are running full speed (or close to it). Blobs are narrow when players run primarily straight ahead (X-axis in the data) and the blobs get wider as player run across the field (Y-axis in the data). Collisions, or more accurately close contacts, between offense and defense players occur when plays occupy the same X,Y coordinate in the data. Vertical bars separate plays. The graph’s Y-axis shows players’ “nfl_id.” The graph’s X-axis shows the frame count. The player tracking operates at 10 frames per second.\nThese are single game snapshots of player activity. As with training load, one day’s information is not very useful. Work remains to generate multi-game graphs for players. It is also important to note how Player Capacity can fluctuate.\nPlayer Capacity increases with physical preparation training. There is a dose-response relationship between training and Capacity. Like medicine, the exercise that occurs during training is helpful and promotes resilience, but only to a point. Too much training can result in overuse, diminishing Capacity and increasing injury risk.\n\n\nDemand-to-Capacity Model: Demand\nGame Demand is mostly under the control of coaches and team management who are in charge of a team’s Game Model and Game Plans. The Game Model refers to a team’s architecture and how they plan to use a player, in large part based on the player’s ability and skill set. Player value is reflected in the salary a player is paid. Game Plans are game-to-game refinements of the Game Model directed at the strengths and weakness of the opponent competition.\nPost-hoc Game Model and Plan analysis can be accomplished by looking at the play-by-play data from games. For example, the throw in football passing plays are either shorter or longer than the distance required for a first down. Receivers are targeted based their ability to get first downs, either via shorter or longer throws.\n\n\\(\\mathrm{prob\\ of\\ catch\\ when\\ } xyac_{fd} &gt; 0.99 =&gt; who\\)\n\\(\\mathrm{prob\\ of\\ catch\\ when\\ } xyac_{fd} &lt; 0.99 * prob\\ expected_{fd}\\)\n\\(x:play\\_type == pass \\ \\&\\& \\ \\mathrm{prob\\ of\\ negative\\ epa\\ } (epa &lt; 0)\\)\n\nThe quarterback of a football has up to six eligible receivers on a passing play. There are also two distinct scenarios on a passing play. Throw past the first down distance so that a catch ensures a first down. Or throw short of first down distance and leave the receiver work to do (in the form of yards after catch, yac) to gain the first down. Each receiver has a probability for getting a first down when the throw distance is less than first down distance (lt_fd) and another probability when the throw distance is greater than first down distance (gt_fd). These probabilities are mutually exclusive but not independent and the added probabilities (pr_gt_fd + pr_lt_fd) is the net probability for a targeted receiver getting a first down.\nThe probability of a fumble by the receiver after a catch is, however, independent. Lost fumble turnovers are rare but costly occurrences for the football offense. The best NFL teams, in many cases, minimize their turnovers.\n\nlibrary(gt)\n\n\n#\\| nflfastr\n\nnflfastR::load_pbp(2025) |&gt; \n  dplyr::group_by(receiver, receiver_id, posteam) |&gt; \n  dplyr::filter(play_type == \"pass\", air_yards &lt; yardline_100, posteam == \"DET\") |&gt; \n  dplyr::mutate(neg_play = ifelse(\n    sack == 1, 1, ifelse(\n      fumble_lost == 1, 1, ifelse(\n        interception == 1, 1, 0)))) |&gt; \n  dplyr::mutate(exp_fd = ifelse(xyac_fd &gt;= 0.99, 1, 0)) |&gt; \n  dplyr::mutate(actual_exp_fd = ifelse(xyac_fd &gt;= 0.99 & first_down, 1, 0)) |&gt; \n  dplyr::mutate(not_fd = ifelse(xyac_fd &lt; 0.99, 1, 0)) |&gt; \n  dplyr::mutate(actual_not_fd = ifelse(xyac_fd &lt; 0.99 & first_down, 1, 0)) |&gt; \n  dplyr::summarize( \n    passes = dplyr::n(), \n    neg_pct = sum(neg_play) / dplyr::n(), \n    gt_com = sum(exp_fd*(complete_pass)), \n    gt_fd = sum(exp_fd, na.rm=TRUE), \n    actual_gt_fd = sum(actual_exp_fd, na.rm=TRUE), \n    gt_fd_pct = actual_gt_fd / gt_fd, \n    lt_com = sum(not_fd*(complete_pass)), \n    lt_fd = sum(not_fd, na.rm=TRUE), \n    actual_lt_fd = sum(actual_not_fd, na.rm=TRUE), \n    lt_fd_pct = actual_lt_fd / lt_fd ) |&gt; \n  dplyr::ungroup() |&gt; \n  dplyr::select(receiver, posteam, passes, neg_pct, gt_com, gt_fd, actual_gt_fd, gt_fd_pct, lt_com, lt_fd, actual_lt_fd, lt_fd_pct) |&gt; \n  gt()\n\n`summarise()` has grouped output by 'receiver', 'receiver_id'. You can override\nusing the `.groups` argument.\n\n\n\n\n\n\n\n\nreceiver\nposteam\npasses\nneg_pct\ngt_com\ngt_fd\nactual_gt_fd\ngt_fd_pct\nlt_com\nlt_fd\nactual_lt_fd\nlt_fd_pct\n\n\n\n\nA.Firkser\nDET\n2\n0.00000000\n0\n0\n0\nNaN\n2\n2\n1\n0.5000000\n\n\nA.St. Brown\nDET\n110\n0.02727273\n26\n46\n26\n0.5652174\n52\n64\n18\n0.2812500\n\n\nB.Wright\nDET\n21\n0.04761905\n0\n0\n0\nNaN\n13\n21\n6\n0.2857143\n\n\nD.Montgomery\nDET\n25\n0.00000000\n2\n3\n2\n0.6666667\n19\n22\n7\n0.3181818\n\n\nI.TeSlaa\nDET\n8\n0.00000000\n3\n6\n3\n0.5000000\n1\n2\n0\n0.0000000\n\n\nJ.Gibbs\nDET\n66\n0.00000000\n3\n7\n3\n0.4285714\n55\n59\n19\n0.3220339\n\n\nJ.Williams\nDET\n71\n0.01408451\n26\n44\n26\n0.5909091\n19\n27\n9\n0.3333333\n\n\nK.Raymond\nDET\n19\n0.00000000\n4\n5\n4\n0.8000000\n11\n14\n3\n0.2142857\n\n\nR.Dwelley\nDET\n4\n0.00000000\n0\n0\n0\nNaN\n2\n4\n0\n0.0000000\n\n\nS.LaPorta\nDET\n48\n0.00000000\n12\n17\n12\n0.7058824\n27\n31\n10\n0.3225806\n\n\nT.Kennedy\nDET\n4\n0.00000000\n1\n1\n1\n1.0000000\n3\n3\n1\n0.3333333\n\n\nNA\nDET\n18\n0.00000000\nNA\n0\n0\nNaN\nNA\n0\n0\nNaN\n\n\n\n\n\n\n\nIn the table “gt_” refers to passes that travel greater than the distance to the first down. “lt_” refers to passes that travel less than the distance to the first down. “neg_pct” refers to percent of passes that are caught and then fumbled to the other team. The principal receivers for DET are A.St.Brown (ASB), J.Gibbs (JG), J.Williams (JW) and S.LaPorta (SLP).\n\ndata.frame( receiver = c(\"A.St.Brown\",\"J.Gibbs\",\"J.Williams\",\"S.LaPorta\"), abbrev = c(\"ASB\", \"JG\", \"JW\", \"SLP\"), gt_fd = c(0.79, 0.33, 0.47, 0.69), lt_fd = c(0.35, 0.18, 0.55, 0.26), fumble = c(0.02, 0, 0.03, 0) ) |&gt; \n  gt()\n\n\n\n\n\n\n\nreceiver\nabbrev\ngt_fd\nlt_fd\nfumble\n\n\n\n\nA.St.Brown\nASB\n0.79\n0.35\n0.02\n\n\nJ.Gibbs\nJG\n0.33\n0.18\n0.00\n\n\nJ.Williams\nJW\n0.47\n0.55\n0.03\n\n\nS.LaPorta\nSLP\n0.69\n0.26\n0.00\n\n\n\n\n\n\n\n“epa” stands for Expected Points Added. Fumble turnovers and first downs have an average epa.\n\nnflfastR::load_pbp(2025) |&gt; \n  dplyr::filter(play_type == \"pass\", air_yards &lt; yardline_100, fumble_lost == 1 | first_down == 1) |&gt; \n  dplyr::summarize( epa_fumble = sum(epa * fumble_lost) / sum(fumble_lost), epa_fd = sum(epa* first_down) / sum(first_down)) |&gt; \n  gt()\n\n\n\n\n\n\n\nepa_fumble\nepa_fd\n\n\n\n\n-4.046483\n1.631217\n\n\n\n\n\n\n\n\n\\(\\mathrm{expected\\ return\\ on\\ throw\\ beyond\\ first\\ down} = P[\\mathrm{gt\\_fd}] * epa_{fd} + P[\\mathrm{fumble}] * epa_{fumble}\\)\n\\(\\mathrm{expected\\ return\\ on\\ throw\\ short\\ of\\ first\\ down} = P[\\mathrm{lt\\_fd}] * epa_{fd} + P[\\mathrm{fumble}] * epa_{fumble}\\)\n\n\n\n\nreceiver\ngt_fd epa\nlt_fd epa\n\n\n\n\nASB\n1.18\n0.48\n\n\nJG\n0.52\n0.28\n\n\nJW\n0.63\n0.76\n\n\nSLP\n1.10\n0.46\n\n\n\nDetroit has two receivers, A.St.Brown and S.LaPorta, who are good at getting first downs on longer throws and on shorter throws. J.Williams is a threat on short throws but struggles to catch longer throws. J.Gibbs is a reliable target on shorter throws but converts a small percentage of those passes into first downs.\n\n\nDemand-to-Capacity Model: Ratio Function\nUse the ratio model for failure function.\n\\[failure function = \\frac{Capacity}{Demand}\\] \\[failure = \\frac{Capacity}{Demand} &lt; 1\\] \\[improvement \\mathrm{\\ or\\ } decline = \\frac{\\sum_{0}^{n}{\\frac{Capacity}{Demand}}}{n}\\]\nIt should be possible to model injuries with a Demand-to-Capacity model but it is probably more effective to use it to model a player’s decline."
  },
  {
    "objectID": "posts/assignment5/index.html",
    "href": "posts/assignment5/index.html",
    "title": "Assignment 5 – Brad Stenger",
    "section": "",
    "text": "1. Prove that if \\(X\\) is a Gaussian random variable with zero mean and unit variance then \\(Y=X^2\\) is a Chi-square random variable.\n\\[\n\\begin{aligned}\n\\mathrm{for \\ } y &lt; 0 \\mathrm{\\ the \\ CDF}, F_Y(y) = P(Y &lt; y) = 0 \\\\\n\\mathrm{for \\ } y &gt;= 0 \\mathrm{\\ the \\ CDF},  F_Y(y) = P(Y &lt; y) = P(X^2 &lt; y) = P(|X| &lt; \\sqrt{y}) = P(-\\sqrt{y} &lt; X &lt; \\sqrt{y}) \\\\\n= F_X(\\sqrt{y}) - F_X(-\\sqrt{-y}) = F_X(\\sqrt{y}) - (1 - F_X(\\sqrt{y})) = 2F_X(\\sqrt{y}) - 1 \\\\\n\\mathrm{PDF}, f_Y(y) = \\frac{d}{dy}F_Y(y) = 2 \\frac{d}{dy}F_X(\\sqrt{y}) - 0 \\\\\n\\mathrm{density \\ function \\ } F_X(\\sqrt{y}) = \\int_{-\\infty}^{\\sqrt{y}} \\frac{1}{\\sqrt{2\\pi}} e^{\\frac{-u^2}{2}} du \\\\\nf_Y(y) = 2 \\frac{d}{dy} \\int_{-\\infty}^{\\sqrt{y}} \\frac{1}{\\sqrt{2\\pi}} e^{\\frac{-u^2}{2}} du \\\\\n= 2 \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{y}{2}} (\\frac{1}{2}y^{-\\frac{1}{2}}) \\\\\n\\Gamma(\\frac{1}{2}) = \\sqrt{\\pi} \\\\\n\\mathrm{Chi\\ square: \\ } f_Y(y) = \\frac{1}{2^{\\frac{1}{2}}\\Gamma(\\frac{1}{2})} y^{-\\frac{1}{2}} e^{-\\frac{y}{2}}\n\\end{aligned}\n\\]\n\n\n2. If \\(X\\) is a uniform random variable in the interval \\([0,2\\pi]\\), what is the probability distribution of:\n\na. \\(Y = X + 1\\)\n\\[\n\\begin{aligned}\n\\mathrm{Y\\ interval: \\ } 0 + 1 &lt;= X + 1 &lt;= 2\\pi + 1 \\\\\n1 &lt;= Y &lt;= 2\\pi + 1 \\\\\n\\mathrm{CDF,\\ X:\\ } F_X(x) = \\int_{0}^{x}\\frac{1}{2\\pi}du = \\frac{x}{2\\pi} \\\\\n\\mathrm{CDF,\\ Y:\\ } F_Y(y) = F_X(y-1) = \\frac{y-1}{2\\pi} \\mathrm{\\ for\\ } 1 &lt;= y &lt;= 2\\pi + 1 \\\\\n\\mathrm{PDF,\\ Y:\\ } f_Y(y) = \\frac{d}{dy}F_Y(y) = \\frac{d}{dy}(\\frac{y-1}{2\\pi}) = \\frac{1}{2\\pi} \\mathrm{\\ for\\ } 1 &lt;= y &lt;= 2\\pi + 1\n\\end{aligned}\n\\]\n\nn_samples &lt;- 1000\nX &lt;- runif(n_samples, min = 0, max = 2 * pi)\nY &lt;- X + 1\nplot(X,Y, cex=0.25, col=\"gray\")\npar(new=T)\nplot(seq(0,2*pi, length.out=n_samples), rep(1/2*pi, times=n_samples), cex=0.25)\npar(new=T)\nplot(rep(1/2*pi, times=n_samples), seq(1, 1+2*pi, length.out=n_samples), cex=0.25, col=\"red\")\n\n\n\n\n\n\n\n\n\n\nb. \\(Y = X^2\\)\n\\[\n\\begin{aligned}\n\\mathrm{Y\\ interval: \\ } 0 &lt;= X^2 &lt;= (2\\pi)^2 \\\\\n0 &lt;= Y &lt;= 4\\pi^2 \\\\\n\\mathrm{CDF,\\ X:\\ } F_X(x) = \\int_{0}^{x}\\frac{1}{2\\pi}du = \\frac{x}{2\\pi} \\\\\n\\mathrm{CDF,\\ Y:\\ } F_Y(y) = F_X(\\sqrt{y}) = \\frac{\\sqrt{y}}{2\\pi} \\mathrm{\\ for\\ } 0 &lt;= y &lt;= 4\\pi^2 \\\\\n\\mathrm{PDF,\\ Y:\\ } f_Y(y) = \\frac{d}{dy}F_Y(y) = \\frac{d}{dy}(\\frac{\\sqrt{y}}{2\\pi}) \\\\\n= \\frac{1}{2\\pi} \\frac{d}{dy}y^{\\frac{1}{2}} \\\\\n= \\frac{1}{2\\pi} \\frac{1}{2} y^{-\\frac{1}{2}} \\\\\n= \\frac{1}{4\\pi\\sqrt{y}} \\mathrm{\\ for\\ } 1 &lt;= y &lt;= 2\\pi + 1\n\\end{aligned}\n\\]\n\nn_samples &lt;- 1000\nX &lt;- runif(n_samples, min = 0, max = 2 * pi)\nY &lt;- X^2\nplot(X,Y, cex=0.25, xlim=c(-2*pi,2*pi), ylim=c(0,4*pi*pi), col=\"gray\")\npar(new=T)\nx &lt;- seq(0,2*pi, length.out=n_samples)\nplot(x, x/(2*pi), xlim=c(-2*pi,2*pi), ylim=c(0,4*pi*pi), cex=0.25)\npar(new=T)\ny &lt;- seq(0, 4*pi*pi, length.out=n_samples)\nplot(1/(4*pi*sqrt(y)), y, xlim=c(-2*pi,2*pi), ylim=c(0,4*pi*pi), cex=0.25, col=\"red\")\n\n\n\n\n\n\n\n\n\n\nc. \\(Y = sinX\\)\n\\[\n\\begin{aligned}\n\\mathrm{Y\\ interval: \\ } [-1,1] \\\\\n\\mathrm{CDF,\\ X:\\ } F_X(x) = \\int_{0}^{x}\\frac{1}{2\\pi}du = \\frac{x}{2\\pi} \\\\\n\\mathrm{CDF,\\ Y:\\ } F_Y(y) = P(X \\in [0,arcsin(y) or X \\in [\\pi - arcsin(y), 2\\pi] ]) \\\\\n= \\frac{1}{2\\pi}((arcsin(y) - 0) + (2\\pi - (\\pi - arcsin(y)))) \\\\\n= \\frac{1}{2\\pi}(2 arcsin(y) + \\pi) \\\\\n= \\frac{1}{2} + \\frac{arcsin(y)}{\\pi} \\mathrm{\\ for\\ } -1 &lt;= y &lt;= 1 \\\\\n\\mathrm{PDF,\\ Y:\\ } f_Y(y) = \\frac{d}{dy}F_Y(y) = \\frac{d}{dy}(\\frac{1}{2} + \\frac{arcsin(y)}{\\pi}) \\\\\n= \\frac{1}{\\pi} \\frac{d}{dy}(arcsin(y)) \\\\\n= \\frac{1}{\\pi\\sqrt{1-y^2}} \\mathrm{\\ for\\ } -1 &lt;= y &lt;= 1\n\\end{aligned}\n\\]\n\nn_samples = 1000\nX &lt;- runif(n_samples, min = 0, max = 2 * pi)\nY &lt;- sin(X)\nplot(X,Y, cex=0.25, xlim=c(-2*pi,2*pi), ylim=c(-1.5,1.5), col=\"gray\")\npar(new=T)\nx &lt;- seq(0,2*pi, length.out=n_samples)\nplot(x, x/(2*pi), xlim=c(-2*pi,2*pi), ylim=c(-1.5, 1.5), cex=0.25)\npar(new=T)\ny &lt;- seq(-1, 1, length.out=n_samples)\nplot(1/(pi*sqrt(1-y^2)), y, xlim=c(-2*pi,2*pi), ylim=c(1.5, -1.5), cex=0.25, col=\"red\")\n\n\n\n\n\n\n\n\n\n\n\n3. If X is an exponential random variable with parameter \\(\\lambda = 1\\), what is the probability distribution of:\n\na. \\(Y = X + 1\\)\n\\[\n\\begin{aligned}\n\\mathrm{CDF,\\ X:\\ } F_X(x) = P(X &lt;= x) = 1 - e^{-x} \\\\\n\\mathrm{CDF,\\ Y:\\ } F_Y(y) = F_X(y-1) = 1 - e^{-(y-1)} \\mathrm{\\ for\\ } y &gt;= 1 \\\\\n\\mathrm{PDF,\\ Y:\\ } f_Y(y) = \\frac{d}{dy}F_Y(y) = \\frac{d}{dy}(1 - e^{-(y-1)}) \\\\\n= -(-1)e^{-(y-1)} \\\\\n= e^{-(y-1)} \\mathrm{\\ for\\ } y &gt;= 1\n\\end{aligned}\n\\]\n\nn_samples = 1000\nX &lt;- rexp(n_samples, 1)\nY &lt;- X + 1\nplot(X,Y, xlim=c(0,10), ylim=c(0,10), cex=0.25, col=\"gray\")\npar(new=T)\nx &lt;- seq(0, 10, length.out=n_samples)\nplot(x, 1 - exp(-x), xlim=c(0,10), ylim=c(0,10), cex=0.25)\npar(new=T)\ny &lt;- seq(1,10, length.out=n_samples)\nplot(1-exp(-(y-1)), y, xlim=c(0,10), ylim=c(0,10), cex=0.25, col=\"red\")\n\n\n\n\n\n\n\n\n\n\nb. \\(Y = X^2\\)\n\\[\n\\begin{aligned}\n\\mathrm{CDF,\\ X:\\ } F_X(x) = P(X &lt;= x) = 1 - e^{-x} \\\\\n\\mathrm{CDF,\\ Y:\\ } F_Y(y) = F_X(\\sqrt{y}) = 1 - e^{-\\sqrt{y}} \\mathrm{\\ for\\ } y &gt;= 0 \\\\\n\\mathrm{PDF,\\ Y:\\ } f_Y(y) = \\frac{d}{dy}F_Y(y) = \\frac{d}{dy}(1 - e^{-\\sqrt{y}}) \\\\\n\\mathrm{chain\\ rule:\\ } \\frac{d}{du}e^u = e^u * u' \\mathrm{\\ where\\ } u = -\\sqrt{y} \\\\\nf_Y(y) = -e^{-\\sqrt{y}} \\frac{d}{dy}(-\\sqrt{y}) \\\\\n= -e^{-\\sqrt{y}} (- \\frac{1}{2\\sqrt{y}}) \\\\\n= \\frac{1}{2\\sqrt{y}} e^{-\\sqrt{y}} \\mathrm{\\ for\\ } y &gt;= 0\n\\end{aligned}\n\\]\n\nn_samples = 1000\nX &lt;- rexp(n_samples, 1)\nY &lt;- X^2\nplot(X,Y, xlim=c(0,10), ylim=c(0,10), cex=0.25, col=\"gray\")\npar(new=T)\nx &lt;- seq(0, 10, length.out=n_samples)\nplot(x, 1 - exp(-x), xlim=c(0,10), ylim=c(0,10), cex=0.25)\npar(new=T)\ny &lt;- seq(1,10, length.out=n_samples)\nplot((1/(2*sqrt(y)))*exp(-(y-1)), y, xlim=c(0,10), ylim=c(0,10), cex=0.25, col=\"red\")\n\n\n\n\n\n\n\n\n\n\nc. \\(Y = X^3\\)\n\\[\n\\begin{aligned}\n\\mathrm{CDF,\\ X:\\ } F_X(x) = P(X &lt;= x) = 1 - e^{-x} \\\\\n\\mathrm{CDF,\\ Y:\\ } F_Y(y) = F_X(y^{\\frac{1}{3}}) = 1 - e^{-y^{\\frac{1}{3}}} \\mathrm{\\ for\\ } y &gt;= 0 \\\\\n\\mathrm{PDF,\\ Y:\\ } f_Y(y) = \\frac{d}{dy}F_Y(y) = \\frac{d}{dy}(1 - e^{-y^{\\frac{1}{3}}}) \\\\\n= -e^{-y^{\\frac{1}{3}}} \\frac{d}{dy}(-y^{\\frac{1}{3}}) \\\\\n= -e^{-y^{\\frac{1}{3}}} (- \\frac{1}{3}) y^{-\\frac{2}{3}} \\\\\n= \\frac{1}{3} y^{-\\frac{2}{3}} e^{-y^{\\frac{1}{3}}} \\mathrm{\\ for\\ } y &gt;= 0\n\\end{aligned}\n\\]\n\nn_samples = 1000\nX &lt;- rexp(n_samples, 1)\nY &lt;- X^3\nplot(X,Y, xlim=c(0,10), ylim=c(0,10), cex=0.25, col=\"gray\")\npar(new=T)\nx &lt;- seq(0, 10, length.out=n_samples)\nplot(x, 1 - exp(-x), xlim=c(0,10), ylim=c(0,10), cex=0.25)\npar(new=T)\ny &lt;- seq(1,10, length.out=n_samples)\nplot((1/3)*(y^(-2/3))*exp(-y^(1/3)), y, xlim=c(0,10), ylim=c(0,10), cex=0.25, col=\"red\")\n\n\n\n\n\n\n\n\n\n\n\n4. If \\(X\\) is a Gaussian random variable with zero mean and unit variance, what is the probability distribution of:\n\n\n\\(Y = e^X\\)\n\\[\n\\begin{aligned}\n\\mathrm{PDF,\\ X:\\ } f_X(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}} \\\\\n\\mathrm{Inverse\\ X:\\ } X = ln(Y) \\\\\n\\mathrm{PDF,\\ Y:\\ } f_Y(y) = f_X(ln(y)) \\frac{d}{dy}ln(y) \\\\\n= f_X(ln(y)) \\frac{1}{y} \\\\\n= \\frac{1}{y \\sqrt{2\\pi}} e^{-\\frac{(ln(y))^2}{2}} \\mathrm{\\ for\\ } y &gt;= 0\n\\end{aligned}\n\\]\n\ncurve(dnorm(x, 0, 1), -4, 4, ylim=c(0,4))\nY &lt;- seq(-4, 4, length.out=10000)\nmeanlog &lt;- 0\nsdlog &lt;- 1\npar(new=TRUE)\nplot((1 / (Y * sdlog * sqrt(2 * pi))) * exp(-((log(Y) - meanlog)^2 / (2 * sdlog^2))), \n     Y,\n     xlim=c(-4,4), \n     ylim=c(0,4), \n     cex=0.1, \n     col=\"gray\")\n\nWarning in log(Y): NaNs produced\n\npar(new=TRUE)\nplot(dlnorm(Y, meanlog=0, sdlog = 1), Y, xlim=c(-4,4), ylim=c(0,4), cex=0.1, col=\"red\")\n\n\n\n\n\n\n\n\n\n2. \\(Y = e^{-X}\\)\n\\[\n\\begin{aligned}\n\\mathrm{PDF,\\ X:\\ } f_X(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}} \\\\\n\\mathrm{Inverse\\ X:\\ } X = -ln(Y) \\\\\n\\mathrm{PDF,\\ Y:\\ } f_Y(y) = f_X(-ln(y)) \\frac{d(-ln(y))}{dy} \\\\\n= f_X(ln(y)) (-\\frac{1}{y}) \\\\\n= -\\frac{1}{y \\sqrt{2\\pi}} e^{-\\frac{(ln(y))^2}{2}} \\mathrm{\\ for\\ } y &gt;= 0\n\\end{aligned}\n\\]\n\ncurve(dnorm(x, 0, 1), -4, 4, ylim=c(0,4))\nY &lt;- seq(-4, 4, length.out=10000)\nmeanlog &lt;- 0\nsdlog &lt;- 1\npar(new=TRUE)\nplot(-1*(1 / (Y * sdlog * sqrt(2 * pi))) * exp(-((log(Y) - meanlog)^2 / (2 * sdlog^2))), \n     Y,\n     xlim=c(-4,4), \n     ylim=c(0,4), \n     cex=0.1, \n     col=\"gray\")\n\nWarning in log(Y): NaNs produced\n\npar(new=TRUE)\nplot(-1*dlnorm(Y, meanlog=0, sdlog = 1), Y, xlim=c(-4,4), ylim=c(0,4), cex=0.1, col=\"red\")\n\n\n\n\n\n\n\n\n\n\n3. \\(Y = |X|\\)\n\ncurve(dnorm(x, 0, 1), -4, 4, ylim=c(-0.1,4))\nsd &lt;- 1\nY &lt;- seq(0,4, length.out=1000)\nX &lt;- (sqrt(2) / (sd * sqrt(pi))) * exp(-Y^2 / (2 * sd^2))\npar(new=TRUE)\nplot(X, Y, type=\"l\", xlim=c(-4,4), ylim=c(-0.1,4), col=\"red\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "analysis",
    "section": "",
    "text": "Assignment 5 – Brad Stenger\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nproject\n\n\n\n\n\n\n\n\nBrad Stenger\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 4 – Brad Stenger\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 3 – Brad Stenger\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nOct 6, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 3, 2025\n\n\nTristan O’Malley\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/assignment4/index.html",
    "href": "posts/assignment4/index.html",
    "title": "Assignment 4 – Brad Stenger",
    "section": "",
    "text": "1. An event occurs in average once every 5 years. Compute the following probabilities:\n\na. The event occurs once in any given year.\n1/5 matches rate, \\(\\lambda = 0.2\\)\n\n\nb. 2 events occur in a 5-year period\n\\(\\lambda = 1\\)\n\ndpois(2, 1)\n\n[1] 0.1839397\n\n\n\n\nc. 5 events occur in a 5-year period\n\ndpois(5,1)\n\n[1] 0.003065662\n\n\n\n\nd. No events occur in a 10-year period\n\ndpois(0,1)*dpois(0,1)\n\n[1] 0.1353353\n\n\n\n\ne. No events occur in a 20-year period\n\ndpois(0,1)^4\n\n[1] 0.01831564\n\n\n\n\nf. 1 event occurs in a 50-year period\n\n0.2*(dpois(0,1)^9)\n\n[1] 2.468196e-05\n\n\n\n\n\n2. What is the probability that a 100-year flood occurs twice in one decade?\n\\(1 - p(\\mathrm{1\\ or\\ 0\\ floods\\ occur})\\)\n\n1 - ppois(0.1, 0.01)\n\n[1] 0.009950166\n\n\n\n\n3. The probability of occurrence (p) of a certain event is most likely around 0.10, but you are not sure and you estimate that it could be as low as 0.02 and as high as 0.30. If you observe 2 occurrences in 10 trials, what does that tell you about p. What is the probability that p is between 0.15 and 0.25?\n\\(\\mathrm{Beta\\ distribution\\ for\\ prior,\\ } \\mu = 0.10 = \\frac{\\alpha}{\\alpha + \\beta}\\)\n\\(\\beta = 0.9\\alpha \\mathrm{,\\ as\\ integers\\ } \\beta = 18 \\mathrm{\\ and\\ } \\alpha = 2\\)\n\nplot.new()\ncurve(dbeta(x, shape1=2, shape2=18), from=0, to=1)\n\n\n\n\n\n\n\n\nPosterior for Beta distribution\n\\(\\alpha' = 2 + 2 = 4 \\mathrm{\\ and\\ } \\beta' = 18 + 10 - 2 = 26\\)\n\nplot.new()\ncurve(dbeta(x, shape1=2, shape2=18), from=0, to=1)\ncurve(dbeta(x, shape1=4, shape2=26), from=0, to=1, col='red', add=TRUE)\n\n\n\n\n\n\n\n\n\\(\\mu' = \\frac{\\alpha'}{\\alpha' + \\beta'} = \\frac{4}{4 + 26}\\)\n\nintegrate(dbeta, 0.15, 0.25, shape1=4, shape2=26)\n\n0.3031957 with absolute error &lt; 3.4e-15\n\n\n\nx_values &lt;- seq(0,1,length.out = 100)\nbeta_densities &lt;- dbeta(x_values, shape1=4, shape2=26)\nplot.new()\ncurve(dbeta(x, shape1=2, shape2=18), from=0, to=1)\ncurve(dbeta(x, shape1=4, shape2=26), from=0, to=1, col='red', add=TRUE)\nshade_indices &lt;- which(x_values &gt;= 0.15 & x_values &lt;= 0.25)\npolygon(c(x_values[shade_indices], rev(x_values[shade_indices])), c(beta_densities[shade_indices], rep(0, length(shade_indices))), col=\"red\", border=NA)\n\n\n\n\n\n\n\n\n\n\n4. Do a literature search determine the probability that 2 hurricanes category 4 or larger hit the eastern U.S. on any single year.\nIn 2017 Hurricane Harvey (Texas) and Hurricane Irma (Florida) made landfall in the eastern U.S.\nSince 2000 there have been 61 Atlantic Basin hurricanes reaching Category 4 or Category 5. Most hurricanes do not make landfall. An even smaller percentage of hurricanes that make landfall hit the U.S.\n\n\n\nYear\nCat 4-5\nLandfall\nLF USA\n\n\n\n\n2000\n2\n0\n0\n\n\n2001\n2\n2\n0\n\n\n2002\n1\n0\n0\n\n\n2003\n2\n0\n0\n\n\n2004\n4\n1\n1\n\n\n2005\n5\n3\n0\n\n\n2006\n0\n0\n0\n\n\n2007\n2\n2\n0\n\n\n2008\n4\n2\n0\n\n\n2009\n1\n0\n0\n\n\n2010\n4\n0\n0\n\n\n2011\n2\n0\n0\n\n\n2012\n0\n0\n0\n\n\n2013\n0\n0\n0\n\n\n2014\n1\n0\n0\n\n\n2015\n1\n1\n0\n\n\n2016\n2\n1\n0\n\n\n2017\n4\n3\n2\n\n\n2018\n2\n1\n1\n\n\n2019\n2\n1\n0\n\n\n2020\n5\n3\n1\n\n\n2021\n2\n1\n1\n\n\n2022\n2\n1\n1\n\n\n2023\n3\n0\n0\n\n\n2024\n4\n2\n1\n\n\n2025\n4\n1\n0\n\n\n\n13/28 of Category 4-5 hurricanes made landfall since 2017, and 7/13 Category 4-5 hurricanes that have made landfall hit the U.S.\n\\(\\lambda = 7 \\mathrm{\\ hurricanes}/9 \\mathrm{\\ years} = 0.78\\)\n\ndpois(2,7/9)\n\n[1] 0.1389621\n\n\n\n\n5. How many magnitude 6.0 (or larger) earthquakes have occurred in the state of California in the last 100 years. Based on this number determine the probability of seeing zero magnitude 6.0 (or larger) in a period of 20 years.\nAccording to https://www.conservation.ca.gov/cgs/earthquakes/big there have been 49 earthquake in or near the state of California since 1925.\n\\(\\lambda = 49 \\mathrm{\\ earthquakes}/100 \\mathrm{\\ years} = 0.49\\)\nSo, for 1 year and for 20 consecutive years\n\ndpois(0, 0.49)\n\n[1] 0.6126264\n\n\n\ndpois(0, 0.49)^20\n\n[1] 5.54516e-05\n\n\n\n\n6. Define the PDF of a Gaussian random variable with zero mean and variance of 4 and then find the PDF of a Laplacian random variable which has the same mean and variance as the Gaussian. Plot your results and comment on their behavior close to the mean and far away from the mean.\n\nlibrary(greybox)\n\nPackage \"greybox\", v2.0.6 loaded.\n\n\n\nx &lt;- seq(-10,10,length.out=1000)\npdf_g &lt;- dnorm(x, 0, 2)\nplot(x, pdf_g, type=\"l\")\nscale = sqrt(2)\npdf_l &lt;- dlaplace(x, 0, scale)\npar(new=T)\nplot(x, pdf_l, type=\"l\", col=\"red\")\n\n\n\n\n\n\n\n\n\n\n7. Generate 10 samples of a Gaussian random variable with mean 10 and variance of 4. Assume you know the mean precisely. Use Bayesian estimation to estimate its variance. Use a uniform prior between 0 and 10.\n\nlibrary(pracma)\n\n\n#uniform distribution\nsigma &lt;- seq(0,10, length.out=10)\nfl &lt;- dunif(sigma, 0, 10)\nplot(sigma,fl,type=\"l\")\n\nvariance = 4\nsd = sqrt(variance)\nsamples &lt;- rnorm(10, 10, sd)\nfor (sample in samples) {\n  likelihood &lt;- (1/sqrt(2*pi*(sigma^2)))*exp(-((sample^2)/(2*(sigma^2))))\n  posterior &lt;- likelihood * fl\n  evidence &lt;- trapz(posterior)*0.1\n  post &lt;- posterior/evidence\n  par(new=T)\n  plot(sigma, posterior,type=\"l\")\n  fl = posterior\n}\npar(new=T)\nplot(sigma,posterior,type=\"l\",col=\"red\")\n\n\n\n\n\n\n\n\n\n\n8. Generate 10 samples of an exponential random variable with \\(\\lambda = 2\\). Use Bayesian estimation to estimate \\(\\lambda\\). Use a uniform prior between 0 and 5. Compare your answer with just using basic statistics.\n\nlibrary(pracma)\n\n\n#uniform distribution\nlambda &lt;- seq(0,5, length.out=10)\nprior &lt;- c(dunif(lambda, 0, 5)[1:5],dunif(lambda, 0, 5)[1:5])\nplot(lambda,prior,type=\"l\")\n\nrate = 2\nsamples &lt;- rexp(10,rate)\nfor (sample in samples) {\n  likelihood &lt;- lambda*exp(-lambda*sample)\n  evidence &lt;- trapz(likelihood*prior)*0.2\n  posterior &lt;- likelihood*prior/evidence\n  par(new=T)\n  plot(lambda,prior,type=\"l\", xlim=c(0,5))\n  prior &lt;- posterior\n}\npar(new=T)\nplot(lambda,posterior,type=\"l\", xlim=c(0,5), col=\"red\")\n\n\n\n\n\n\n\n\n\nsummary(samples)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.01876 0.11832 0.17124 0.26569 0.26082 1.05484 \n\n\n\nsummary(posterior)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00000 0.02811 0.57117 0.53853 0.97659 1.19530 \n\n\n\n\n9. Generate 10 samples of uniform random variable with \\(a = 0\\) and \\(b = 1\\). Assume \\(a\\) is known, use Bayesian estimation to estimate \\(b\\). Use a Gaussian prior for \\(b\\) with a mean of 1.5 and variance of 0.25.\n\nlibrary(pracma)\n\n\nsamples = runif(10)\nprior = rnorm(10, 1.5, 0.5)\na = 0\nb = max(prior)\nplot(0, b, xlim=c(0,10), ylim=c(0,4))\n\nfor (i in 1:length(samples)) {\n  likelihood &lt;- 1/(b-a)\n  evidence &lt;- ((1/samples[i]) + (1/(b-a)))/2\n  posterior &lt;- likelihood*prior/evidence\n  b &lt;- max(posterior)\n  prior &lt;- posterior\n  par(new=T)\n  plot(i, b, xlim=c(0,10), ylim=c(0,4))\n}\n\n\n\n\n\n\n\n\n\nsamples = runif(10)\nprior = rnorm(10, 1.5, 0.5)\na = 0\nb = max(prior)\nplot(0, b, xlim=c(0,10), ylim=c(0,4))\n\nfor (i in 1:length(samples)) {\n  likelihood &lt;- 1/(b-a)\n  evidence &lt;- ((1/samples[i]) + (1/(b-a)))/2\n  posterior &lt;- likelihood*prior/evidence\n  b &lt;- max(posterior)\n  prior &lt;- posterior\n  par(new=T)\n  plot(i, b, xlim=c(0,10), ylim=c(0,4))\n}"
  }
]